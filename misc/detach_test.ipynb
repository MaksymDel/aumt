{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.FloatTensor([5])\n",
    "\n",
    "fc1 = nn.Linear(1, 2)\n",
    "fc2 = nn.Linear(2, 1)\n",
    "opt1 = optim.Adam(fc1.parameters(),lr=1e-1)\n",
    "opt2 = optim.Adam(fc2.parameters(),lr=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBefore\n\nweight tensor([[  2.3851],\n        [-17.0823]])\nbias tensor([ 0.4770, -3.4165])\nweight tensor([[-9.5406, 21.7013]])\nbias tensor([-10.3396])\n\nAfter\n\nweight tensor([[0.],\n        [0.]])\nbias tensor([0., 0.])\nweight tensor([[-9.5406, 21.7013]])\nbias tensor([-10.3396])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z)\n",
    "cost = (x_p - x) ** 2\n",
    "'''\n",
    "print (z)\n",
    "print (x_p)\n",
    "print (cost)\n",
    "'''\n",
    "opt1.zero_grad()\n",
    "opt2.zero_grad()\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "print('\\nBefore\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "opt1.zero_grad()\n",
    "opt2.zero_grad()\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z.detach())\n",
    "cost = (x_p - x) ** 2\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "print('\\nAfter\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt3 = optim.Adam(list(fc1.parameters()) + list(fc2.parameters()),lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBefore\n\nweight tensor([[  2.3851],\n        [-17.0823]])\nbias tensor([ 0.4770, -3.4165])\nweight tensor([[-9.5406, 21.7013]])\nbias tensor([-10.3396])\n\nAfter\n\nweight tensor([[0.],\n        [0.]])\nbias tensor([0., 0.])\nweight tensor([[-9.5406, 21.7013]])\nbias tensor([-10.3396])\n"
     ]
    }
   ],
   "source": [
    "z = fc1(x)\n",
    "x_p = fc2(z)\n",
    "cost = (x_p - x) ** 2\n",
    "'''\n",
    "print (z)\n",
    "print (x_p)\n",
    "print (cost)\n",
    "'''\n",
    "opt3.zero_grad()\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "print('\\nBefore\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "opt3.zero_grad()\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z.detach())\n",
    "cost = (x_p - x) ** 2\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "print('\\nAfter\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBefore\n\nweight tensor([[  5.9901],\n        [-66.8719]])\nbias tensor([  1.1980, -13.3744])\nweight tensor([[ 88.0734, -76.4363]])\nbias tensor([-168.0059])\n\nBefore\n\nweight tensor([[  6.7389],\n        [-75.2309]])\nbias tensor([  1.3478, -15.0462])\nweight tensor([[ 93.5780, -81.2135]])\nbias tensor([-178.5063])\n\nAfter\n\nweight tensor([[  6.7389],\n        [-75.2309]])\nbias tensor([  1.3478, -15.0462])\nweight tensor([[ 99.0826, -85.9908]])\nbias tensor([-189.0066])\n"
     ]
    }
   ],
   "source": [
    "opt3.zero_grad()\n",
    "\n",
    "print('\\nBefore\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z.detach())\n",
    "cost = (x_p - x) ** 2\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z)\n",
    "cost = (x_p - x) ** 2\n",
    "'''\n",
    "print (z)\n",
    "print (x_p)\n",
    "print (cost)\n",
    "'''\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "print('\\nBefore\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z.detach())\n",
    "cost = (x_p - x) ** 2\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "\n",
    "print('\\nAfter\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "    \n",
    "opt3.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBegin0\n\nweight tensor([[  10.4827],\n        [-117.0258]])\nbias tensor([  2.0965, -23.4052])\nweight tensor([[ 170.6423, -148.0953]])\nbias tensor([-325.5114])\n\nBefore\n\nweight tensor([[  11.2315],\n        [-125.3848]])\nbias tensor([  2.2463, -25.0770])\nweight tensor([[ 176.1469, -152.8725]])\nbias tensor([-336.0117])\n\nAfter\n\nweight tensor([[  11.2315],\n        [-125.3848]])\nbias tensor([  2.2463, -25.0770])\nweight tensor([[ 181.6515, -157.6498]])\nbias tensor([-346.5121])\n\nEnd:\n\nweight tensor([[  11.2315],\n        [-125.3848]])\nbias tensor([  2.2463, -25.0770])\nweight tensor([[ 181.6515, -157.6498]])\nbias tensor([-346.5121])\n"
     ]
    }
   ],
   "source": [
    "opt3.zero_grad()\n",
    "\n",
    "print('\\nBefore\\n')\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z)\n",
    "cost1 = (x_p - x) ** 2\n",
    "\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "z = fc1(x)\n",
    "x_p = fc2(z.detach())\n",
    "cost = (x_p - x) ** 2\n",
    "\n",
    "'''\n",
    "print (z)\n",
    "print (x_p)\n",
    "print (cost)\n",
    "'''\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "\n",
    "print('\\nAfter\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "    \n",
    "opt3.step()\n",
    "\n",
    "\n",
    "print('\\nEnd:\\n')\n",
    "\n",
    "for n, p in fc1.named_parameters():\n",
    "    print (n, p.grad.data)\n",
    "\n",
    "\n",
    "for n, p in fc2.named_parameters():\n",
    "    print (n, p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
